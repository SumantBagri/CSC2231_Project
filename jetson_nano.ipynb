{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ldm_uncond.latent_diffusion_uncond import LDMPipeline\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from eval.power import PowerReader\n",
    "\n",
    "DTYPE = torch.float16\n",
    "DEVICE = torch.device('cuda')\n",
    "# Init PowerReader\n",
    "pr = PowerReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init pipeline, move to device and warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep = 000\r"
     ]
    }
   ],
   "source": [
    "diffusion_pipeline = LDMPipeline().to(device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "diffusion_pipeline.eval()\n",
    "diffusion_pipeline.warmup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create noise input\n",
    "noise = torch.randn((1, 3, 64, 64), dtype=DTYPE, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number 0\n",
      "peak memory: 1893.34 MiB, increment: 56.62 MiB\n",
      "POM_5V_IN  max \t =====> \t 6606 mW\n",
      "POM_5V_CPU max \t =====> \t 3734 mW\n",
      "POM_5V_GPU max \t =====> \t 1328 mW\n",
      "POM_5V_IN  avg \t =====> \t 6182 mW\n",
      "POM_5V_CPU avg \t =====> \t 3023 mW\n",
      "POM_5V_GPU avg \t =====> \t 859 mW\n",
      "Iteration number 1\n",
      "peak memory: 1830.27 MiB, increment: 50.84 MiB\n",
      "POM_5V_IN  max \t =====> \t 6606 mW\n",
      "POM_5V_CPU max \t =====> \t 3698 mW\n",
      "POM_5V_GPU max \t =====> \t 1436 mW\n",
      "POM_5V_IN  avg \t =====> \t 6008 mW\n",
      "POM_5V_CPU avg \t =====> \t 2896 mW\n",
      "POM_5V_GPU avg \t =====> \t 860 mW\n",
      "Iteration number 2\n",
      "peak memory: 1788.05 MiB, increment: 13.10 MiB\n",
      "POM_5V_IN  max \t =====> \t 6606 mW\n",
      "POM_5V_CPU max \t =====> \t 3704 mW\n",
      "POM_5V_GPU max \t =====> \t 1472 mW\n",
      "POM_5V_IN  avg \t =====> \t 6203 mW\n",
      "POM_5V_CPU avg \t =====> \t 3046 mW\n",
      "POM_5V_GPU avg \t =====> \t 889 mW\n",
      "Iteration number 3\n",
      "peak memory: 1762.84 MiB, increment: 2.78 MiB\n",
      "POM_5V_IN  max \t =====> \t 6582 mW\n",
      "POM_5V_CPU max \t =====> \t 3704 mW\n",
      "POM_5V_GPU max \t =====> \t 1579 mW\n",
      "POM_5V_IN  avg \t =====> \t 6286 mW\n",
      "POM_5V_CPU avg \t =====> \t 3063 mW\n",
      "POM_5V_GPU avg \t =====> \t 950 mW\n",
      "Iteration number 4\n",
      "peak memory: 1759.73 MiB, increment: 24.15 MiB\n",
      "POM_5V_IN  max \t =====> \t 6582 mW\n",
      "POM_5V_CPU max \t =====> \t 3662 mW\n",
      "POM_5V_GPU max \t =====> \t 1292 mW\n",
      "POM_5V_IN  avg \t =====> \t 6291 mW\n",
      "POM_5V_CPU avg \t =====> \t 3082 mW\n",
      "POM_5V_GPU avg \t =====> \t 934 mW\n",
      "Iteration number 5\n",
      "peak memory: 1757.62 MiB, increment: 21.44 MiB\n",
      "POM_5V_IN  max \t =====> \t 6582 mW\n",
      "POM_5V_CPU max \t =====> \t 3662 mW\n",
      "POM_5V_GPU max \t =====> \t 1148 mW\n",
      "POM_5V_IN  avg \t =====> \t 6288 mW\n",
      "POM_5V_CPU avg \t =====> \t 3073 mW\n",
      "POM_5V_GPU avg \t =====> \t 931 mW\n",
      "Iteration number 6\n",
      "peak memory: 1757.07 MiB, increment: 24.10 MiB\n",
      "POM_5V_IN  max \t =====> \t 6570 mW\n",
      "POM_5V_CPU max \t =====> \t 3596 mW\n",
      "POM_5V_GPU max \t =====> \t 1184 mW\n",
      "POM_5V_IN  avg \t =====> \t 6244 mW\n",
      "POM_5V_CPU avg \t =====> \t 3035 mW\n",
      "POM_5V_GPU avg \t =====> \t 943 mW\n",
      "Iteration number 7\n",
      "peak memory: 1757.47 MiB, increment: 24.15 MiB\n",
      "POM_5V_IN  max \t =====> \t 6582 mW\n",
      "POM_5V_CPU max \t =====> \t 3704 mW\n",
      "POM_5V_GPU max \t =====> \t 1113 mW\n",
      "POM_5V_IN  avg \t =====> \t 6287 mW\n",
      "POM_5V_CPU avg \t =====> \t 3075 mW\n",
      "POM_5V_GPU avg \t =====> \t 937 mW\n",
      "Iteration number 8\n",
      "peak memory: 1757.07 MiB, increment: 23.39 MiB\n",
      "POM_5V_IN  max \t =====> \t 6534 mW\n",
      "POM_5V_CPU max \t =====> \t 3590 mW\n",
      "POM_5V_GPU max \t =====> \t 1579 mW\n",
      "POM_5V_IN  avg \t =====> \t 6257 mW\n",
      "POM_5V_CPU avg \t =====> \t 3037 mW\n",
      "POM_5V_GPU avg \t =====> \t 946 mW\n",
      "Iteration number 9\n",
      "peak memory: 1755.50 MiB, increment: 21.48 MiB\n",
      "POM_5V_IN  max \t =====> \t 6570 mW\n",
      "POM_5V_CPU max \t =====> \t 3626 mW\n",
      "POM_5V_GPU max \t =====> \t 1292 mW\n",
      "POM_5V_IN  avg \t =====> \t 6297 mW\n",
      "POM_5V_CPU avg \t =====> \t 3040 mW\n",
      "POM_5V_GPU avg \t =====> \t 967 mW\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-571d06d5ffe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_vals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fm' is not defined"
     ]
    }
   ],
   "source": [
    "## Generate sample\n",
    "fl = open(\"output/eval_data/jetson_latency.txt\", \"a\")\n",
    "# fm = open(\"output/eval_data/jetson_memory.txt\", \"a\")\n",
    "fp = open(\"output/eval_data/jetson_power.txt\", \"a\")\n",
    "for i in range(10):\n",
    "    print(f\"\\rIteration number {i}\")\n",
    "    print(f\"\\nRun number {i}: \\n\", file=fl)\n",
    "    # print(f\"\\nRun number {i}: \\n\", file=fm)\n",
    "    print(f\"\\nRun number {i}: \\n\", file=fp)\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            pr.start()\n",
    "            %memit sample = diffusion_pipeline(noise)\n",
    "            pr.stop()\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=5), file=fl)\n",
    "    # print(sout.stdout, file=fm)\n",
    "    print(pr.print_vals(), file=fp)\n",
    "fl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample.cpu().float().numpy()[0]/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT Compiled Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL ONLY ONCE!!\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device('cpu')\n",
    "\n",
    "diffusion_pipeline = LDMPipeline().to(device=DEVICE, dtype=DTYPE)\n",
    "noise = torch.randn((1, 3, 64, 64), dtype=DTYPE, device=DEVICE)\n",
    "print(\"Diffusion Model loaded!\")\n",
    "\n",
    "\n",
    "print(\"Starting JIT Trace...\")\n",
    "# JIT Trace and Save model\n",
    "torchscript_model = torch.jit.trace(diffusion_pipeline, noise)\n",
    "print(\"JIT Trace finished! Saving Model...\")\n",
    "torch.jit.save(torchscript_model, \"uldm_jit.ptl\")\n",
    "print(\"JIT compiled model saved!\")\n",
    "\n",
    "del torchscript_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load optimized UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_diffusion_pipeline = LDMPipeline()\n",
    "\n",
    "optimized_diffusion_pipeline = optimized_diffusion_pipeline.to(device=DEVICE, dtype=DTYPE)\n",
    "optimized_diffusion_pipeline.load_optimized_unet(\"uldm_unet_fp16_sim.ts\")\n",
    "optimized_diffusion_pipeline.eval()\n",
    "optimized_diffusion_pipeline.warmup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from optimized network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Generate sample\n",
    "\n",
    "noise = torch.randn((1, 3, 64, 64), dtype=DTYPE, device=DEVICE)\n",
    "# with torch.cuda.amp.autocast():\n",
    "%timeit %memit sample = optimized_diffusion_pipeline(noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize sample from optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample.cpu().float().numpy()[0]/255)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
