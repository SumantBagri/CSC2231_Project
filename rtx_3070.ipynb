{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/y2k/anaconda3/envs/csc2231/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from ldm_uncond.latent_diffusion_uncond import LDMPipeline\n",
    "from eval.evaluation import BaseEvaluator, ONNXEvaluator\n",
    "\n",
    "# Define hardware\n",
    "dev = \"rtx_3070\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - 32bit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "Warmup complete!\n",
      "Evaluation iter: 0\n",
      "Evaluation iter: 1\n",
      "Timestep = 000\r"
     ]
    }
   ],
   "source": [
    "# Init evaluator\n",
    "baseline_evaluator= BaseEvaluator(dev=dev, perf_cls=\"baseline\")\n",
    "\n",
    "# Init inputs and model\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device('cuda')\n",
    "diffusion_pipeline = LDMPipeline().to(device=DEVICE, dtype=DTYPE)\n",
    "\n",
    "noise = torch.randn((1, 3, 64, 64), dtype=DTYPE, device=DEVICE)\n",
    "\n",
    "# Evaluate the baseline\n",
    "baseline_evaluator.evaluate(diffusion_pipeline,noise)\n",
    "\n",
    "del diffusion_pipeline\n",
    "del baseline_evaluator\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 1 - JIT Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init evaluator\n",
    "optim1_evaluator= BaseEvaluator(dev=dev, perf_cls=\"32bit_jit\")\n",
    "\n",
    "# Init inputs and model\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "noise = torch.randn((1, 3, 64, 64), dtype=DTYPE, device=DEVICE)\n",
    "torchscript_model = torch.jit.load(\"output/optim/model_jit_fp32_cuda.ptl\")\n",
    "\n",
    "# Evaluate optimization-1\n",
    "with torch.no_grad():\n",
    "    optim1_evaluator.evaluate(torchscript_model,noise)\n",
    "\n",
    "del torchscript_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 2 - Quantization (16-bit) + JIT Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init evaluator\n",
    "optim2_evaluator= BaseEvaluator(dev=dev, perf_cls=\"16bit_jit\")\n",
    "\n",
    "# Init inputs and model\n",
    "DTYPE = torch.float16\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "noise = torch.randn((1, 3, 64, 64), dtype=DTYPE, device=DEVICE)\n",
    "torchscript_model = torch.jit.load(\"output/optim/model_jit_fp16_cuda.ptl\")\n",
    "\n",
    "# Evaluate optimization-2\n",
    "with torch.no_grad():\n",
    "    optim2_evaluator.evaluate(torchscript_model,noise)\n",
    "\n",
    "del torchscript_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 3 - ONNX Runtime (Graph optimizations + Transformer specific optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init inputs and model\n",
    "DTYPE = torch.float32\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "noise = torch.randn((1, 3, 64, 64), dtype=DTYPE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init evaluator\n",
    "optim3_1 = ONNXEvaluator(\"output/optim/model_onnx_fp32_cpu.onnx\", dev=dev, perf_cls=\"onnx_vanilla\")\n",
    "optim3_1.evaluate(noise)\n",
    "\n",
    "del optim3_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized ONNX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init evaluator\n",
    "optim3_2 = ONNXEvaluator(\"output/optim/model_onnx_fp32_cpu_optimized.onnx\", dev=dev, perf_cls=\"onnx_optim\")\n",
    "optim3_2.evaluate(noise)\n",
    "\n",
    "del optim3_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Optimized ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init evaluator\n",
    "optim3_3 = ONNXEvaluator(\"output/optim/model_onnx_fp32_cpu_optimized_tf.onnx\", dev=dev, perf_cls=\"onnx_optim_tf\")\n",
    "optim3_3.evaluate(noise)\n",
    "\n",
    "del optim3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add output/eval_data/\n",
    "!git commit -m \"Adding evaluation pipeline and notebooks\"\n",
    "!git push"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization 4 - TensorRT (Layer & Tensor fusion + Quantization (16-bit) + JIT Compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load optimized UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init evaluator\n",
    "optim1_evaluator= BaseEvaluator(dev=dev, perf_cls=\"32bit_jit\")\n",
    "\n",
    "optimized_diffusion_pipeline = LDMPipeline()\n",
    "\n",
    "optimized_diffusion_pipeline = optimized_diffusion_pipeline.to(device=DEVICE, dtype=DTYPE)\n",
    "optimized_diffusion_pipeline.load_optimized_unet(\"uldm_unet_fp16_sim.ts\")\n",
    "optimized_diffusion_pipeline.eval()\n",
    "optimized_diffusion_pipeline.warmup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from optimized network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Generate sample\n",
    "\n",
    "noise = torch.randn((1, 3, 64, 64), dtype=DTYPE, device=DEVICE)\n",
    "# with torch.cuda.amp.autocast():\n",
    "%timeit %memit sample = optimized_diffusion_pipeline(noise)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
